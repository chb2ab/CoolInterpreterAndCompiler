Crispin Bernier
chb2ab
pa2

	I implemented my lexer in Javascript using jison. I began by writing out the regular expressions for the cool special characters and reserved keywords because they were the easiest to write out and test. I then implemented the identifiers, comments, and strings. The only difficulty with handling identifiers was to prioritize them last to prevent keywords from being chosen as identifiers. The most difficult part of handling comments was handling nested comments. To allow for these I had to use Jisons state function. When a nested comment begins the lexer enters a comment state where any additional nested comments increment a counter and comment exits decrement the counter. The lexer can only exit the comment state when the counter is at 0. This functionality allowed for nested comments.

	The most difficult part of handling strings was having to handle backslashed quotation marks. I handled all strings using a single regular expression that ended on a quotation mark but consumed backslashed quotation marks. Because the expression takes the longest matching string, this expression always consumes backslashed quotation marks as long as there is a valid quotation mark to end on. There were some corner cases such as strings ending in an even number of backslashes or strings that were too long that had to be handled in separate checks afterwards, but there is only one regular expression for matching strings. After implementing these regular expressions Jison generated approximately 700 lines of code to implement the lexer. This was difficult to handle because I had to understand how the code worked in order to input and output the data correctly, which involved adding some additional functions to the lexer.

	Testing the lexer involved reading the cool documentation and formulating different test cases. Having a shell script to automatically test my lexer was very helpful for this. My tests generally did not have valid cool structure because it wasn't required to properly test the lexer. My good.cl example is a valid cool lexing example. This example includes many corner cases that my lexer struggled with as it was being developed. For example there are carriage returns without newlines (which I had to enter by ASCII value through vim), nested comments, and strings with backslashed quotation marks, as well as different identifiers and keywords. This test file was good for making sure my lexer was passing a wide variety of cases. My bad.cl file is a negative test case which tests both that the lexer can detect the error and also that it outputs the correct line number. This file targets the nested comment requirement as well as cool keywords and characters. The nested comment requirements was one of the hardest requirements so having many tests for it was important. Developing tests was relatively easy for the lexer because they didn't need to be valid programs. This meant testing my lexer on random line noise was meaningful, and it was easy to create very specific test cases. As I added and tested new features I also kept the old test cases in the file. This way testing new features also tested the old features, which was helpful for me to verify that my implementation was still working.